{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e98faf-e2be-4457-8444-8d32d2e09f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from jiwer import wer\n",
    "import jiwer\n",
    "transformation = jiwer.Compose([\n",
    "    jiwer.ToLowerCase(),\n",
    "    jiwer.RemoveWhiteSpace(replace_by_space=True),\n",
    "    jiwer.RemoveMultipleSpaces(),\n",
    "    jiwer.ReduceToListOfListOfWords(word_delimiter=\" \")\n",
    "    ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a010cda3-5bf5-468e-8cd4-a77a62614149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "\n",
    "audio_path = \"primock57/output/audio_utterances/\"\n",
    "\n",
    "class ClinDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, device=DEVICE, filename):\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.transcripts = {}\n",
    "        with open(filename,'r') as vf:\n",
    "            trans = vf.readlines()\n",
    "        for t in trans:\n",
    "            self.transcripts[t.split('|')[1].strip()] = t.split('|')[0]  \n",
    "        \n",
    "        self.wavpaths = []\n",
    "        for wp in os.listdir(audio_path):\n",
    "            basename = wp[:-4]\n",
    "            try:\n",
    "                t = self.transcripts[basename]\n",
    "                self.wavpaths.append(wp)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print('num files',len(self.wavpaths))\n",
    "        \n",
    "               \n",
    "    def __len__(self):\n",
    "        return len(self.wavpaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        wav_path = os.path.join(audio_path, self.wavpaths[idx])\n",
    "        audio, sample_rate = sf.read(wav_path)\n",
    "        audio = processor(audio, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
    "        \n",
    "        basename = wav_path.split('/')[-1][:-4]\n",
    "        transcript = self.transcripts[basename]\n",
    "        transcript = transcript.upper()\n",
    "        with processor.as_target_processor():\n",
    "            labels = processor(transcript, return_tensors=\"pt\").input_ids\n",
    "        sample = { \"lab\":labels.squeeze(0), \"aud\":audio.squeeze(0), \"trans\":transcript}\n",
    "        return sample\n",
    "\n",
    "\n",
    "    def collate(self, batch):\n",
    "        audios = [item[\"aud\"] for item in batch]\n",
    "        labels = [item[\"lab\"] for item in batch]\n",
    "        trans = [item[\"trans\"] for item in batch]\n",
    "\n",
    "        # Pad audio sequences\n",
    "        audio_batch = pad_sequence(audios, batch_first=True)\n",
    "\n",
    "        # Pad label sequences\n",
    "        labels_batch = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "        # Attention masks for labels\n",
    "        attention_masks = labels_batch != -100\n",
    "\n",
    "        # Create batch dictionary\n",
    "        batch = {\n",
    "            \"input_values\": audio_batch,\n",
    "            \"labels\": labels_batch,\n",
    "            \"trans\": trans\n",
    "        }\n",
    "\n",
    "        return batch\n",
    "\n",
    "transcripts_file = \"primock57/output/train_transcript.ref.txt\"\n",
    "dataset = ClinDataset(transcripts_file)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=4, collate_fn=dataset.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d98d3c-4e25-488a-8404-770d50bad7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.000001)\n",
    "\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136a9ee-9d49-4378-a0f9-5efde170fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_acc_step = 1\n",
    "\n",
    "# FINE-TUNE\n",
    "# 1 epoch\n",
    "step=0\n",
    "for batch in tqdm(loader):\n",
    "    input_values = batch[\"input_values\"]\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    # compute loos and update by passing labels\n",
    "\n",
    "    loss = model(input_values, labels=labels).loss\n",
    "    if torch.isnan(loss):\n",
    "        #loss = torch.zeros_like(loss)\n",
    "        optimizer.zero_grad()\n",
    "        continue\n",
    "    loss = loss/len(input_values)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    if step % grad_acc_step == 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    step+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac953c8-912c-40cc-9483-272b8efda86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wer of fine-tuned model\n",
    "\n",
    "def print_wer(references, hypotheses):\n",
    "    \n",
    "    data = pd.DataFrame(dict(hypothesis=hypotheses, reference=references)) #, phonemes=gtphonms))\n",
    "    print(\"WER:\", wer(list(data[\"reference\"]), list(data[\"hypothesis\"]), truth_transform=transformation, hypothesis_transform=transformation ))\n",
    "\n",
    "    for hyp, ref in zip(data[\"hypothesis\"],data[\"reference\"]):\n",
    "        if hyp!=\"\" and ref!=\"\":\n",
    "            data[\"hypothesis_clean\"] = normalizer(hyp)\n",
    "            data[\"reference_clean\"] = normalizer(ref)                              \n",
    "    print(\"WER without fillers:\", wer(list(data[\"reference_clean\"]), list(data[\"hypothesis_clean\"]), truth_transform=transformation, hypothesis_transform=transformation ))\n",
    "\n",
    "\n",
    "\n",
    "transcripts_file = \"primock57/output/test_transcript.ref.txt\"\n",
    "dataset = ClinDataset(transcripts_file)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "\n",
    "for batch in loader:\n",
    "    input_values = batch[\"input_values\"]\n",
    "    text = batch[\"text\"]\n",
    "    \n",
    "    logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    # transcribe\n",
    "    trans = processor.decode(predicted_ids[0])\n",
    "    \n",
    "    hypotheses.extend(trans)\n",
    "    references.extend(text)\n",
    "\n",
    "print_wer(references, hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebce202-5346-4308-b1ec-417b398d10be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
